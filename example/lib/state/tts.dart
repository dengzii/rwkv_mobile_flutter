part of 'p.dart';

extension _Instruction on Language {
  String get _ttsSpkInstruct => switch (this) {
        Language.none => "",
        Language.en => "",
        Language.ja => "Êó•Êú¨Ë™û„ÅßË©±„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
        Language.ko => "ÌïúÍµ≠Ïñ¥Î°ú ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî.",
        Language.zh_Hans => "",
        Language.zh_Hant => "",
      };
}

extension _TTSStatic on _TTS {
  static const _defaultTextInInput = "";
  static const _cfmStepsKey = "cfmSteps";
  static const _defaultCfmSteps = 5;
  static const _replaceMap = {
    "English": "üá∫üá∏",
    "Japanese": "üáØüáµ",
    "Korean": "üá∞üá∑",
    "Chinese(PRC)": "üá®üá≥",
  };
  static const _spkNameToLanguageMap = {
    "English": Language.en,
    "Japanese": Language.ja,
    "Korean": Language.ko,
    "Chinese(PRC)": Language.zh_Hans,
  };
}

class _TTS {
  late final audioInteractorShown = qs(false);
  late final cfmSteps = qs(_TTSStatic._defaultCfmSteps);
  late final focusNode = FocusNode();
  late final hasFocus = qs(false);
  late final instructions = qsf<TTSInstruction, int?>(null);
  late final interactingInstruction = qs(TTSInstruction.none);
  late final intonationShown = qs(false);
  late final selectSourceAudioPath = qsn<String>();
  late final selectedLanguage = qs(Language.none);

  /// Ëã•Áî®Êà∑ÈÄâÊã©Ëá™Â∑±ÁöÑÂ£∞Èü≥‰Ωú‰∏∫Ê∫êÂ£∞Èü≥, ÂàôËØ• value ‰∏∫ null
  late final selectedSpkName = qsn<String>();

  late final selectedSpkPanelFilter = qs(Language.none);
  late final spkPairs = qs<JSON>({});
  late final spkShown = qs(false);
  late final textEditingController = TextEditingController(text: _TTSStatic._defaultTextInInput);
  late final textInInput = qs(_TTSStatic._defaultTextInInput);
  late final ttsDone = qs(true);

  late final overallProgress = qs(0.0);
  late final perWavProgress = qs<List<double>>([]);
  late final filePaths = qs<List<String>>([]);

  Timer? _queryTimer;
}

/// Private methods
extension _$TTS on _TTS {
  FV _init() async {
    if (P.app.demoType.q != DemoType.tts) return;
    qq;
    P.chat.focusNode.addListener(_onChatFocusNodeChanged);

    textEditingController.addListener(_onTextEditingControllerValueChanged);
    textInInput.l(_onTextChanged);
    await getTTSSpkNames();

    final prefs = await SharedPreferences.getInstance();
    final cfmSteps = prefs.getInt(_TTSStatic._cfmStepsKey);
    if (cfmSteps == null) {
      this.cfmSteps.q = _TTSStatic._defaultCfmSteps;
    } else {
      this.cfmSteps.q = cfmSteps;
    }

    final spkPairs = this.spkPairs.q;

    selectedSpkName.q = spkPairs.keys.where((e) => e.contains("Chinese")).random;

    selectSourceAudioPath.q = null;

    focusNode.addListener(() {
      hasFocus.q = focusNode.hasFocus;
    });

    selectedSpkName.l(_onSelectSpkNameChanged, fireImmediately: true);
    spkShown.l(_onSpkShownChanged, fireImmediately: true);

    P.rwkv.broadcastStream.listen(_onStreamEvent, onDone: _onStreamDone, onError: _onStreamError);
  }

  void _onSpkShownChanged(bool next) {
    selectedSpkPanelFilter.q = selectedLanguage.q;
  }

  void _onSelectSpkNameChanged(String? next) {
    qq;
    if (next == null) {
      selectedLanguage.q = Language.none;
      return;
    }

    for (final key in _TTSStatic._spkNameToLanguageMap.keys) {
      final contains = next.contains(key);
      if (contains) {
        selectedLanguage.q = _TTSStatic._spkNameToLanguageMap[key]!;
        return;
      }
    }
  }

  void _onChatFocusNodeChanged() {
    qqq("P.chat.focusNode.hasFocus: ${P.chat.focusNode.hasFocus}");
    if (P.chat.focusNode.hasFocus) {
      dismissAllShown(intonationShown: intonationShown.q);
    }
  }

  void _onTextChanged(String next) {
    // qqq("_onTextChanged");
    final textInController = textEditingController.text;
    if (next != textInController) textEditingController.text = next;
  }

  void _onTextEditingControllerValueChanged() {
    // qqq("_onTextEditingControllerValueChanged");
    final textInController = textEditingController.text;
    if (textInInput.q != textInController) textInInput.q = textInController;
  }

  void _startQueryTimer() {
    qq;
    _queryTimer = Timer.periodic(const Duration(milliseconds: 200), (timer) => _pulse());
  }

  void _pulse() {
    P.rwkv.send(to_rwkv.GetTTSGenerationProgress());
    P.rwkv.send(to_rwkv.GetPrefillAndDecodeSpeed());
    P.rwkv.send(to_rwkv.GetTTSOutputFileList());
  }

  void _stopQueryTimer() {
    _queryTimer?.cancel();
    _queryTimer = null;
  }

  FV _runTTS({
    required String ttsText,
    required String instructionText,
    required String promptWavPath,
    required String outputWavPath,
    required String promptSpeechText,
  }) async {
    qq;
    if (!ttsDone.q) {
      qqe("ttsDone is true");
      Alert.warning("TTS is running, please wait for it to finish");
      return;
    }

    ttsDone.q = false;

    P.rwkv.send(to_rwkv.StartTTS(
      ttsText: ttsText,
      instructionText: instructionText,
      promptWavPath: promptWavPath,
      outputWavPath: outputWavPath,
      promptSpeechText: promptSpeechText,
    ));

    filePaths.q = [];
    perWavProgress.q = [];
    overallProgress.q = 0.0;
    ttsDone.q = false;

    final receiveId = P.chat.receiveId.q;

    if (receiveId != null) {
      P.chat._updateMessageById(
        id: receiveId,
        changing: false,
      );
    }

    _stopQueryTimer();
    _startQueryTimer();
  }

  void _onStreamEvent(from_rwkv.FromRWKV event) {
    switch (event) {
      case from_rwkv.TTSResult res:
        _onTTSResult(res);
      case from_rwkv.TTSGenerationProgress res:
        qqq("overallProgress: ${res.overallProgress}");
        qqq("perWavProgress: ${res.perWavProgress}");
      case from_rwkv.TTSGenerationStart res:
        qqq(res);
      case from_rwkv.TTSOutputFileList res:
        qqq(res.outputFileList);
      default:
        break;
    }
  }

  void _onTTSResult(from_rwkv.TTSResult res) async {
    final filePaths = res.filePaths;
    final perWavProgress = res.perWavProgress.map((e) => (e * 100).round() / 100.0).toList();
    final overallProgress = (res.overallProgress * 100).round() / 100.0;

    this.filePaths.q = filePaths;
    this.perWavProgress.q = perWavProgress;
    this.overallProgress.q = overallProgress;

    final allReceived = overallProgress >= 1.0;
    final receiveId = P.chat.receiveId.q;

    if (receiveId == null) {
      qqw("receiveId is null");
      return;
    }

    P.chat._updateMessageById(
      id: receiveId,
      changing: !allReceived,
      ttsOverallProgress: overallProgress,
      ttsPerWavProgress: perWavProgress,
      ttsFilePaths: filePaths,
    );

    if (allReceived) {
      _stopQueryTimer();
      ttsDone.q = true;
      return;
    }
  }

  void _onStreamDone() {
    qq;
  }

  void _onStreamError(Object error, StackTrace stackTrace) {
    qqe("error: $error");
    if (!kDebugMode) Sentry.captureException(error, stackTrace: stackTrace);
  }
}

/// Public methods
extension $TTS on _TTS {
  FV startStateSync() async {
    Timer.periodic(500.ms, (timer) {
      //
    });
  }

  FV stopStateSync() async {
    // timer.cancel();
  }

  FV getTTSSpkNames() async {
    qq;
    try {
      final data = await rootBundle.loadString("assets/lib/chat/pairs.json");
      final spkPairs = await compute(_parseSpkNames, data);
      this.spkPairs.q = spkPairs;
    } catch (e) {
      qqe("$e");
      Sentry.captureException(e, stackTrace: StackTrace.current);
    }
  }

  FV onAudioInteractorButtonPressed() async {
    qq;
    Gaimon.light();
    if (focusNode.hasFocus) focusNode.unfocus();
    if (P.chat.focusNode.hasFocus) P.chat.focusNode.unfocus();
    audioInteractorShown.q = !audioInteractorShown.q;
    if (audioInteractorShown.q) {
      intonationShown.q = false;
      spkShown.q = false;
    }
  }

  FV onSpkButtonPressed() async {
    qq;
    Gaimon.light();
    if (focusNode.hasFocus) focusNode.unfocus();
    if (P.chat.focusNode.hasFocus) P.chat.focusNode.unfocus();
    spkShown.q = !spkShown.q;
    if (spkShown.q) {
      audioInteractorShown.q = false;
      intonationShown.q = false;
    }
  }

  FV onIntonationButtonPressed() async {
    qq;
    Gaimon.light();
    if (focusNode.hasFocus) focusNode.unfocus();
    if (P.chat.focusNode.hasFocus) P.chat.focusNode.unfocus();
    intonationShown.q = !intonationShown.q;
    if (intonationShown.q) {
      audioInteractorShown.q = false;
      spkShown.q = false;
    }

    if (intonationShown.q) {
      P.chat.focusNode.unfocus();
      await Future.delayed(300.ms);
      P.chat.focusNode.requestFocus();
    } else {
      if (P.chat.focusNode.hasFocus) P.chat.focusNode.unfocus();
    }
  }

  @Deprecated("ÊÉ≥ÊÉ≥Êõ¥Èù¢ÂêëÁä∂ÊÄÅÁöÑÊñπÊ≥ï")
  String safe(String input) {
    const replaceMap = {};

    String name = input;
    replaceMap.forEach((key, value) {
      name = name.replaceAll(key, value);
    });

    name = name.replaceAll(name.split("_").first + "_", "");

    name = name.replaceAll(RegExp(r"_[0-9]+"), "");

    return name;
  }

  @Deprecated("ÊÉ≥ÊÉ≥Êõ¥Èù¢ÂêëÁä∂ÊÄÅÁöÑÊñπÊ≥ï")
  String flagChange(String input) {
    String name = input;
    _TTSStatic._replaceMap.forEach((key, value) {
      name = name.replaceAll(key, value);
    });

    return name;
  }

  Future<String> getPrebuiltSpkAudioPathFromTemp(String spkName) async {
    qq;
    final fileName = "$spkName.wav";
    final path = "assets/lib/chat/$fileName";
    final localPath = await fromAssetsToTemp(path);
    return localPath;
  }

  Future<String> getPromptSpeechText(String spkName) async {
    qq;
    final fileName = "$spkName.json";
    final data = await rootBundle.loadString("assets/lib/chat/$fileName");
    final json = HF.json(jsonDecode(data));
    return json["transcription"];
  }

  FV gen() async {
    qq;
    if (P.rwkv.currentModel.q == null) {
      P.fileManager.modelSelectorShown.q = true;
      return;
    }

    if (!P.chat.canSend.q) return;

    late final Message? msg;
    final id = HF.debugShorterUS;
    final receiveId = HF.debugShorterUS + 1;
    final spkName = selectedSpkName.q;

    if (spkName == null && this.selectSourceAudioPath.q == null) {
      Alert.warning("Please select a spk or a wav file");
      return;
    }

    final promptSpeechText = spkName == null ? "" : await getPromptSpeechText(spkName);
    final selectSourceAudioPath = this.selectSourceAudioPath.q ?? await getPrebuiltSpkAudioPathFromTemp(spkName!);
    final ttsText = P.chat.textEditingController.text;

    String instructionText = textInInput.q;

    if (instructionText.isEmpty) instructionText = selectedLanguage.q._ttsSpkInstruct;

    final outputWavPrefix = P.app.cacheDir.q!.path + "/$receiveId.output";
    // TODO: handle multiple wav output via getTTSOutputFileList
    final outputWavPath = P.app.cacheDir.q!.path + "/$receiveId.output.0.wav";

    if (ttsText.isEmpty) {
      Alert.warning("Please enter text to generate TTS");
      return;
    }

    P.chat.textEditingController.text = "";
    P.chat.focusNode.unfocus();

    audioInteractorShown.q = false;
    intonationShown.q = false;
    spkShown.q = false;

    P.chat.textEditingController.clear();

    msg = Message(
      id: id,
      content: "",
      isMine: true,
      type: MessageType.userTTS,
      isReasoning: false,
      paused: false,
      ttsTarget: ttsText,
      ttsSpeakerName: spkName,
      ttsSourceAudioPath: selectSourceAudioPath,
      ttsInstruction: instructionText,
      audioUrl: selectSourceAudioPath,
      ttsCFMSteps: cfmSteps.q,
    );

    P.chat.messages.ua(msg);

    Future.delayed(34.ms).then((_) {
      P.chat.scrollToBottom();
    });

    final receiveMsg = Message(
      id: receiveId,
      content: ttsText,
      isMine: false,
      changing: true,
      isReasoning: false,
      paused: false,
      type: MessageType.ttsGeneration,
      audioUrl: outputWavPath,
      ttsOverallProgress: 0.0,
      ttsPerWavProgress: const [],
      ttsFilePaths: const [],
    );

    P.chat.receiveId.q = receiveId;
    P.chat.messages.ua(receiveMsg);

    qqr("""ttsText: $ttsText
instructionText: $instructionText
promptWavPath: $selectSourceAudioPath
promptSpeechText: $promptSpeechText
outputWavPath: $outputWavPath""");

    await _runTTS(
      ttsText: ttsText,
      instructionText: instructionText,
      promptWavPath: selectSourceAudioPath,
      promptSpeechText: promptSpeechText,
      outputWavPath: outputWavPrefix,
    );
  }

  void dismissAllShown({bool intonationShown = false}) {
    qqq("intonationShown: $intonationShown");

    if (P.app.demoType.q != DemoType.tts) return;

    audioInteractorShown.q = false;
    spkShown.q = false;
    this.intonationShown.q = intonationShown;

    focusNode.unfocus();
    interactingInstruction.q = TTSInstruction.none;
  }

  void onRefreshButtonPressed() {
    qq;
    textInInput.q = _TTSStatic._defaultTextInInput;
    TTSInstruction.values.forEach((action) {
      instructions(action).q = null;
    });
  }

  void onClearButtonPressed() {
    qq;
    textInInput.q = "";
    TTSInstruction.values.forEach((action) {
      instructions(action).q = null;
    });
  }

  void syncInstruction() {
    qq;
    String instruction = "ËØ∑Áî®";
    TTSInstruction.values.where((e) => e.forInstruction).forEach((action) {
      final index = instructions(action).q;
      if (index != null) {
        instruction += "${action.head}${action.options[index]}${action.tail}";
      }
    });
    instruction += "ËØ¥‰∏Ä‰∏ã";
    instruction = instruction.replaceAll("Áî®Áî®", "Áî®");
    instruction = instruction.replaceAll("Áî®‰ª•", "‰ª•");
    instruction = instruction.replaceAll("Áî®Ê®°‰ªø", "Ê®°‰ªø");
    textInInput.q = instruction;
    textEditingController.text = instruction;
  }

  FV setTTSCFMSteps(int steps) async {
    qq;
    cfmSteps.q = steps;
    P.rwkv.send(to_rwkv.SetTTSCFMSteps(steps));
    final prefs = await SharedPreferences.getInstance();
    prefs.setInt(_TTSStatic._cfmStepsKey, steps);
  }

  FV showTTSCFMStepsSelector() async {
    qq;
    final context = getContext();
    if (context == null) return;
    if (!context.mounted) return;
    final currentQ = cfmSteps.q;
    final res = await showConfirmationDialog<int?>(
      context: context,
      title: "TTS CFM Steps",
      message: "ËåÉÂõ¥3ÔΩû10ÂêßÔºåË∂äÈ´òË∂äÊÖ¢Ë∂äÁ≤æÁªÜ",
      initialSelectedActionKey: currentQ,
      actions: [3, 4, 5, 6, 7, 8, 9, 10].m((value) => AlertDialogAction<int>(label: value.toString(), key: value)),
    );
    qqq("$res");

    if (res == null) return;

    cfmSteps.q = res;
    final sp = await SharedPreferences.getInstance();
    await sp.setInt(_TTSStatic._cfmStepsKey, res);
    setTTSCFMSteps(res);
  }
}

JSON _parseSpkNames(String message) {
  return HF.json(jsonDecode(message));
}
